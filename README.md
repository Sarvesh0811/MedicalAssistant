# ğŸ©º AI Medical Assistant â€” RAG-based Backend API

![Medical Assistant Chatbot Interface](assets/medicalassistant.png)

## ğŸ§  Project Overview

This project is a **Medical Domain AI Assistant Backend** built using **Retrieval-Augmented Generation (RAG)**.  
It allows users to upload medical PDFs (textbooks, reports, notes) and ask questions that are answered using **context retrieved from the uploaded documents**, reducing hallucinations and improving medical accuracy.

The system is implemented as a **FastAPI backend** exposing REST APIs for document ingestion and question answering.

---

## ğŸ“ What is RAG?

**Retrieval-Augmented Generation (RAG)** combines:
- ğŸ” Information Retrieval (vector search from documents)
- ğŸ¤– Large Language Models (LLMs)

Instead of answering purely from model memory, the system:
1. Retrieves relevant document chunks from a vector database
2. Injects them into the prompt
3. Generates grounded, context-aware answers

This is especially useful for **medical and factual domains**.

---

## ğŸ”„ High-Level Architecture
              User Question

                   â†“

            Question Embedding

                   â†“

        Pinecone Vector Database

                   â†“

        Relevant Document Chunks

                   â†“

     RAG Chain (LangChain + Groq LLM)

                   â†“

              Final Answer


---

## âœ¨ Features

- Upload one or multiple **medical PDF documents**
- Automatic **PDF text extraction**
- Recursive **text chunking**
- Vector embeddings using **Google embeddings**
- Vector storage and similarity search via **Pinecone**
- Context-aware answer generation using **Groq LLaMA3-70B**
- Modular, scalable **FastAPI backend**
- Centralized logging and exception handling

---

## ğŸ§° Tech Stack

| Component | Technology |
|--------|-----------|
| Backend Framework | FastAPI |
| LLM | Groq (LLaMA3-70B) |
| Embeddings | Google Generative AI |
| Vector Database | Pinecone |
| RAG Framework | LangChain |
| Language | Python |
| API Style | REST |

---

## ğŸ“š API Endpoints

### â¤ Upload PDFs
```

POST /upload_pdfs/

```
**Description:**  
Upload one or more PDF files for ingestion into the vector database.

---

### â¤ Ask a Question
```

POST /ask/

```
**Description:**  
Ask a question based on the uploaded documents.

**Form Field:**
- question` â€” User query
```

## ğŸ—‚ï¸ Backend Structure & File Organization

server/
â”œâ”€â”€ main.py                  # FastAPI application entry point
â”œâ”€â”€ logger.py                # Centralized logging configuration
â”œâ”€â”€ test.py                  # Backend testing / sandbox file
â”œâ”€â”€ .env                     # Environment variables (API keys, configs)
â”œâ”€â”€ requirements.txt         # Backend dependencies
â”‚
â”œâ”€â”€ middlewares/             # Custom FastAPI middlewares
â”‚   â””â”€â”€ exception_handlers.py  # Global exception handling middleware
â”‚
â”œâ”€â”€ modules/                 # Core business logic & RAG pipeline
â”‚   â”œâ”€â”€ llm.py               # LLM initialization (Groq, prompts)
â”‚   â”œâ”€â”€ load_vectorstore.py  # Pinecone vector store setup & loading
â”‚   â”œâ”€â”€ pdf_handlers.py      # PDF loading, text extraction & chunking
â”‚   â””â”€â”€ query_handlers.py    # RAG query execution logic
â”‚
â”œâ”€â”€ routes/                  # API route definitions
â”‚   â”œâ”€â”€ upload_pdfs.py       # PDF upload & ingestion endpoints
â”‚   â””â”€â”€ ask_question.py      # Question answering endpoint
â”‚
â”œâ”€â”€ uploaded_docs/           # Uploaded PDF storage
â”‚   â””â”€â”€ *.pdf
â”‚
â””â”€â”€ __pycache__/             # Python cache files

